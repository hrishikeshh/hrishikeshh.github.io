<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Extracting Training Data from Large Language Models | Hrishi </title> <meta name="author" content="Hrishikesh Singh"> <meta name="description" content="Collections of my thoughts, work and notes. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%90&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hrishikeshh.github.io/summaries/2023-08-29-extracting-training-data-from-large-language-models/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Hrishi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item active"> <a class="nav-link" href="/summaries/">ML Paper Summaries <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Notes </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div style="display:none"> $$ \newcommand{\bone}{\mathbf{1}} \newcommand{\bbeta}{\mathbf{\beta}} \newcommand{\bdelta}{\mathbf{\delta}} \newcommand{\bepsilon}{\mathbf{\epsilon}} \newcommand{\blambda}{\mathbf{\lambda}} \newcommand{\bomega}{\mathbf{\omega}} \newcommand{\bpi}{\mathbf{\pi}} \newcommand{\bphi}{\mathbf{\phi}} \newcommand{\bvphi}{\mathbf{\varphi}} \newcommand{\bpsi}{\mathbf{\psi}} \newcommand{\bsigma}{\mathbf{\sigma}} \newcommand{\btheta}{\mathbf{\theta}} \newcommand{\btau}{\mathbf{\tau}} \newcommand{\ba}{\mathbf{a}} \newcommand{\bb}{\mathbf{b}} \newcommand{\bc}{\mathbf{c}} \newcommand{\bd}{\mathbf{d}} \newcommand{\be}{\mathbf{e}} \newcommand{\boldf}{\mathbf{f}} \newcommand{\bg}{\mathbf{g}} \newcommand{\bh}{\mathbf{h}} \newcommand{\bi}{\mathbf{i}} \newcommand{\bj}{\mathbf{j}} \newcommand{\bk}{\mathbf{k}} \newcommand{\bell}{\mathbf{\ell}} \newcommand{\bm}{\mathbf{m}} \newcommand{\bn}{\mathbf{n}} \newcommand{\bo}{\mathbf{o}} \newcommand{\bp}{\mathbf{p}} \newcommand{\bq}{\mathbf{q}} \newcommand{\br}{\mathbf{r}} \newcommand{\bs}{\mathbf{s}} \newcommand{\bt}{\mathbf{t}} \newcommand{\bu}{\mathbf{u}} \newcommand{\bv}{\mathbf{v}} \newcommand{\bw}{\mathbf{w}} \newcommand{\bx}{\mathbf{x}} \newcommand{\by}{\mathbf{y}} \newcommand{\bz}{\mathbf{z}} \newcommand{\bA}{\mathbf{A}} \newcommand{\bB}{\mathbf{B}} \newcommand{\bC}{\mathbf{C}} \newcommand{\bD}{\mathbf{D}} \newcommand{\bE}{\mathbf{E}} \newcommand{\bF}{\mathbf{F}} \newcommand{\bG}{\mathbf{G}} \newcommand{\bH}{\mathbf{H}} \newcommand{\bI}{\mathbf{I}} \newcommand{\bJ}{\mathbf{J}} \newcommand{\bK}{\mathbf{K}} \newcommand{\bL}{\mathbf{L}} \newcommand{\bM}{\mathbf{M}} \newcommand{\bN}{\mathbf{N}} \newcommand{\bP}{\mathbf{P}} \newcommand{\bQ}{\mathbf{Q}} \newcommand{\bR}{\mathbf{R}} \newcommand{\bS}{\mathbf{S}} \newcommand{\bT}{\mathbf{T}} \newcommand{\bU}{\mathbf{U}} \newcommand{\bV}{\mathbf{V}} \newcommand{\bW}{\mathbf{W}} \newcommand{\bX}{\mathbf{X}} \newcommand{\bY}{\mathbf{Y}} \newcommand{\bZ}{\mathbf{Z}} \newcommand{\calA}{\mathcal{A}} \newcommand{\calB}{\mathcal{B}} \newcommand{\calC}{\mathcal{C}} \newcommand{\calD}{\mathcal{D}} \newcommand{\calE}{\mathcal{E}} \newcommand{\calF}{\mathcal{F}} \newcommand{\calG}{\mathcal{G}} \newcommand{\calH}{\mathcal{H}} \newcommand{\calI}{\mathcal{I}} \newcommand{\calJ}{\mathcal{J}} \newcommand{\calK}{\mathcal{K}} \newcommand{\calL}{\mathcal{L}} \newcommand{\calM}{\mathcal{M}} \newcommand{\calN}{\mathcal{N}} \newcommand{\calO}{\mathcal{O}} \newcommand{\calP}{\mathcal{P}} \newcommand{\calQ}{\mathcal{Q}} \newcommand{\calR}{\mathcal{R}} \newcommand{\calS}{\mathcal{S}} \newcommand{\calT}{\mathcal{T}} \newcommand{\calU}{\mathcal{U}} \newcommand{\calV}{\mathcal{V}} \newcommand{\calW}{\mathcal{W}} \newcommand{\calX}{\mathcal{X}} \newcommand{\calY}{\mathcal{Y}} \newcommand{\calZ}{\mathcal{Z}} \newcommand{\R}{\mathbb{R}} \newcommand{\C}{\mathbb{C}} \newcommand{\N}{\mathbb{N}} \newcommand{\Z}{\mathbb{Z}} \newcommand{\F}{\mathbb{F}} \newcommand{\Q}{\mathbb{Q}} \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min} \newcommand{\nnz}[1]{\mbox{nnz}(#1)} \newcommand{\dotprod}[2]{\langle #1, #2 \rangle} \newcommand{\ignore}[1]{} \let\Pr\relax \DeclareMathOperator*{\Pr}{\mathbf{Pr}} \newcommand{\E}{\mathbb{E}} \DeclareMathOperator*{\Ex}{\mathbf{E}} \DeclareMathOperator*{\Var}{\mathbf{Var}} \DeclareMathOperator*{\Cov}{\mathbf{Cov}} \DeclareMathOperator*{\stddev}{\mathbf{stddev}} \DeclareMathOperator*{\avg}{avg} \DeclareMathOperator{\poly}{poly} \DeclareMathOperator{\polylog}{polylog} \DeclareMathOperator{\size}{size} \DeclareMathOperator{\sgn}{sgn} \DeclareMathOperator{\dist}{dist} \DeclareMathOperator{\vol}{vol} \DeclareMathOperator{\spn}{span} \DeclareMathOperator{\supp}{supp} \DeclareMathOperator{\tr}{tr} \DeclareMathOperator{\Tr}{Tr} \DeclareMathOperator{\codim}{codim} \DeclareMathOperator{\diag}{diag} \newcommand{\PTIME}{\mathsf{P}} \newcommand{\LOGSPACE}{\mathsf{L}} \newcommand{\ZPP}{\mathsf{ZPP}} \newcommand{\RP}{\mathsf{RP}} \newcommand{\BPP}{\mathsf{BPP}} \newcommand{\P}{\mathsf{P}} \newcommand{\NP}{\mathsf{NP}} \newcommand{\TC}{\mathsf{TC}} \newcommand{\AC}{\mathsf{AC}} \newcommand{\SC}{\mathsf{SC}} \newcommand{\SZK}{\mathsf{SZK}} \newcommand{\AM}{\mathsf{AM}} \newcommand{\IP}{\mathsf{IP}} \newcommand{\PSPACE}{\mathsf{PSPACE}} \newcommand{\EXP}{\mathsf{EXP}} \newcommand{\MIP}{\mathsf{MIP}} \newcommand{\NEXP}{\mathsf{NEXP}} \newcommand{\BQP}{\mathsf{BQP}} \newcommand{\distP}{\mathsf{dist\textbf{P}}} \newcommand{\distNP}{\mathsf{dist\textbf{NP}}} \newcommand{\eps}{\epsilon} \newcommand{\lam}{\lambda} \newcommand{\dleta}{\delta} \newcommand{\simga}{\sigma} \newcommand{\vphi}{\varphi} \newcommand{\la}{\langle} \newcommand{\ra}{\rangle} \newcommand{\wt}[1]{\widetilde{#1}} \newcommand{\wh}[1]{\widehat{#1}} \newcommand{\ol}[1]{\overline{#1}} \newcommand{\ul}[1]{\underline{#1}} \newcommand{\ot}{\otimes} \newcommand{\zo}{\{0,1\}} \newcommand{\co}{:} %\newcommand{\co}{\colon} \newcommand{\bdry}{\partial} \newcommand{\grad}{\nabla} \newcommand{\transp}{^\intercal} \newcommand{\inv}{^{-1}} \newcommand{\symmdiff}{\triangle} \newcommand{\symdiff}{\symmdiff} \newcommand{\half}{\tfrac{1}{2}} \newcommand{\bbone}{\mathbbm 1} \newcommand{\Id}{\bbone} \newcommand{\SAT}{\mathsf{SAT}} \newcommand{\bcalG}{\boldsymbol{\calG}} \newcommand{\calbG}{\bcalG} \newcommand{\bcalX}{\boldsymbol{\calX}} \newcommand{\calbX}{\bcalX} \newcommand{\bcalY}{\boldsymbol{\calY}} \newcommand{\calbY}{\bcalY} \newcommand{\bcalZ}{\boldsymbol{\calZ}} \newcommand{\calbZ}{\bcalZ} $$ </div> <div class="publications"> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row no-count"> <div id="2012.07805v2" class="col-sm-12"> <div class="title"><h2> <a href="http://arxiv.org/abs/2012.07805v2" rel="external nofollow noopener" target="_blank"> Extracting Training Data from Large Language Models </a> </h2></div> <div class="author"> Nicholas Carlini, Florian Tramer, Eric Wallace, and <span class="more-authors" title="click to view 9 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '9 more authors' ? 'Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, Colin Raffel' : '9 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">9 more authors</span> </div> <div class="periodical"> <em></em> Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2012.07805v2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2012.07805v2.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <h3>Paper Abstract</h3> <div class="abstract"> <p>It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model’s training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">2012.07805v2</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and Oprea, Alina and Raffel, Colin}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Extracting Training Data from Large Language Models}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2012.07805v2}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CR}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://arxiv.org/abs/2012.07805v2}</span><span class="p">,</span>
  <span class="na">file</span> <span class="p">=</span> <span class="s">{2012.07805v2.pdf}</span><span class="p">,</span>
  <span class="na">eprintnover</span> <span class="p">=</span> <span class="s">{2012.07805}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <div class="post"> <article class="post-content"> <div id="markdown-content"> <h3 id="three-important-things">Three Important Things</h3> <h4 id="1-training-data-extraction-attack">1. Training Data Extraction Attack</h4> <p>In this paper, the authors come up with and investigate the efficacy of a training data extraction attack on large language models. To mitigate the potential harms of the study, they performed this on GPT-2, where both the training dataset and models are already public. They used the largest GPT-2 model with 1.5 billion parameters.</p> <p>An example of personally identifiable information that they managed to extract is given below (with permission from the individual affected):</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/summaries/llm-extraction-example-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/summaries/llm-extraction-example-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/summaries/llm-extraction-example-1400.webp"></source> <img src="/assets/img/summaries/llm-extraction-example.webp" class="z-depth-1 center" width="350px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The table below shows the types of memorized content that they were able to recover:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/summaries/llm-extraction-memorized-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/summaries/llm-extraction-memorized-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/summaries/llm-extraction-memorized-1400.webp"></source> <img src="/assets/img/summaries/llm-extraction-memorized.webp" class="z-depth-1 center" width="350px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Since it is possible that many common text phrases will be memorized by the language model (such as the MIT license), the authors are concerned only about uncovering eidetic memorization. This is data that is memorized despite only appearing a very small number of times.</p> <p>Formally, they define \(k\)-eidetic memorization for a string \(s\) as being able to recover \(s\) from the language model using a “reasonable” prompt, when it only appears at most \(k\) times in the training dataset. There is some nuance here about whether a prompt is reasonable as there are pathological corner cases where you could simply ask the prompt to repeat a string back to you. This was not mentioned in the paper, but I believe a reasonable working definition could be that there is information gain (in a Shannon entropy manner) when I see the result, conditioned on the prompt.</p> <p>The high-level workflow of their attack is as follows.</p> <ol> <li>They provide prompts (potentially just the empty start-of-sentence token) to the GPT-2 language model, and auto-regressively sample from it</li> <li>Use one of 6 metrics to determine the likelihood that each of the generations contains memorized content. These will be discussed later.</li> <li>The generated texts are de-duplicated</li> <li>The top 100 generations (according to the metric) are chosen</li> <li>The researchers manually verified whether the generations are indeed memorized by an internet search, with confirmation with OpenAI (who developed GPT-2)</li> </ol> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/summaries/llm-extraction-workflow-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/summaries/llm-extraction-workflow-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/summaries/llm-extraction-workflow-1400.webp"></source> <img src="/assets/img/summaries/llm-extraction-workflow.webp" class="z-depth-1 center" width="600px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h4 id="2-text-generation-schemes">2. Text Generation Schemes</h4> <p>The authors initially faced the problem of having a low diversity of outputs when just performing top-\(n\) sampling autoregressively on an empty prompt. To increase the precision and recall of generated text that corresponds to eidetic memorization, they found two techniques that helped:</p> <ol> <li> <p>Sampling with a Decaying Temperature: Borrowing ideas from simulated tempering, they initially start sampling with a higher temperature which then decays throughout the generation process. Decaying the temperature is necessary to eventually extract high-confidence sequences.</p> </li> <li> <p>Conditioning on Internet text: The authors curated a dataset that was a cleaned subset of the <a href="https://commoncrawl.org/" rel="external nofollow noopener" target="_blank">Common Crawl</a>, and used short samples from this dataset as prompts. They found that using Internet-sourced text as prompts helped to elicit a larger diversity of responses from GPT-2, which was also trained on Internet data.</p> </li> </ol> <h4 id="3-membership-inference">3. Membership Inference</h4> <p>After generating 200,000 samples, it is very much infeasible to try to evaluate them all by hand to determine whether they were memorized, and hence the authors used different heuristics as metrics to determine the likelihood that a piece of generated sample was eidetically memorized.</p> <ol> <li>Comparing to other neural language models: they compared the perplexity of the output from the largest 1.5 billion GPT-2 XL model against its smaller counterparts: Small (117M parameters) and Medium (345M parameters). If a piece of output was not surprising to the XL model but was surprising to either the Small or Medium model, then it likely indicates that the output was \(k\)-eidetic memorized and is not a product of general natural language.</li> <li>Comparing to <a href="https://www.zlib.net/" rel="external nofollow noopener" target="_blank">zlib</a> compression: a common failure mode was the propensity for the text to spit out repetitive text which has low perplexity, despite this being very unnatural in real life, such as the repeated text “I love you. I love you. I love you…”. To catch this case, the authors applied <code class="language-plaintext highlighter-rouge">zlib</code> compression on the output, and used the ratio of perplexity to <code class="language-plaintext highlighter-rouge">zlib</code> compression entropy as a metric.</li> <li>Comparing to lowercased text: they measured the perplexity of both the original output, and when it is completely lowercased. If the lowercased output has much higher perplexity (indicating the casing was important), then it hints that it was likely memorized.</li> <li>Perplexity on sliding window: In some cases only a part of the output has low perplexity and contains memorized text, while other parts could have low likelihood. This is especially so if simulated tempering was used. As such, the minimum perplexity averaged over a sliding window of 50 tokens was used.</li> </ol> <p>The number of memorized samples recovered using each of these metrics and text generation strategies are given below:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/summaries/llm-extraction-results-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/summaries/llm-extraction-results-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/summaries/llm-extraction-results-1400.webp"></source> <img src="/assets/img/summaries/llm-extraction-results.webp" class="z-depth-1 center" width="600px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="most-glaring-deficiency">Most Glaring Deficiency</h3> <p>The different metrics for membership inference are orthogonal in some sense that they each have different strengths. I feel that the authors could have experimented with combining the metrics, weighted in some appropriate manner, to obtain even higher rates of recovering eidetically memorized content.</p> <h3 id="conclusions-for-future-work">Conclusions for Future Work</h3> <p>The results of this paper are an understatement of the true extent to which training data can be recovered from black-box querying of large language models. As models get larger, this is only going to become an even bigger issue.</p> <p>This also shows that concerns from companies using foundation models for their internal use about not having their confidential data trained on are extremely valid, and per-organization fine-tuned models to avoid information leakage may be fundamentally unavoidable unless there is a way to train in a privacy-preserving manner without incurring a huge computational tax.</p> </div> </article> <p class="post-meta">Written 2023</p> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Hrishikesh Singh. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script> </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let theme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===theme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=document.querySelector(".navbar-collapse");e.classList.contains("show")&&e.classList.remove("show"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"Publications",description:"Papers under review/submission and pre-prints are available upon request.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-ml-paper-summaries",title:"ML Paper Summaries",description:"",section:"Navigation",handler:()=>{window.location.href="/summaries/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-notes",title:"Notes",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-sample-test",title:"Sample Test",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/sample/"}},{id:"post-reading",title:"Reading",description:"A personal reading list.",section:"Posts",handler:()=>{window.location.href="/blog/2024/library/"}},{id:"post-research-my-two-cents",title:"Research: My two cents",description:"These are the nuggets of wisdom I found in the corners of internet while doing my own stint of research. Enjoy.",section:"Posts",handler:()=>{window.location.href="/blog/2024/research/"}},{id:"post-how-to-write-a-review-paper",title:"How to write a review paper?",description:"A walk through guide for structuring your research",section:"Posts",handler:()=>{window.location.href="/blog/2024/review-paper/"}},{id:"post-dsa-templates-java",title:"DSA templates (Java)",description:"These are java code templates based on most common datastructures and algorithm problems.",section:"Posts",handler:()=>{window.location.href="/blog/2024/java-template/"}},{id:"post-pythonic-templates",title:"Pythonic Templates",description:"A collection of code templates for common patterns in data structures and algorithms",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-template/"}},{id:"post-dynamic-programming",title:"Dynamic Programming",description:"Setting up the foundations of Dynamic programming along with a generic framework to disintegrate the problem into components",section:"Posts",handler:()=>{window.location.href="/blog/2024/dynamic-prog/"}},{id:"post-deep-learning",title:"Deep Learning",description:"A curated list of resources for a comprehensive understanding of deep learning.  ",section:"Posts",handler:()=>{window.location.href="/blog/2024/deep-learning/"}},{id:"post-the-beauty-of-latex",title:"The Beauty of LaTeX",description:"When was the first time you had to use LaTeX? If you are like most people, it was probably suddenly forced upon you during your first math or CS class where you had to start writing proofs, with minimal guidance on how to get started. Unfortunately, this meant that while many people have good operational knowledge of LaTeX, there are still many small mistakes and best practices which are not followed, which are not corrected by TAs as they are either not severe enough to warrant a note, or perhaps even the TAs themselves are not aware of them.  In this post, we cover some common mistakes that are made by LaTeX practitioners (even in heavily cited papers), and how to address them.",section:"Posts",handler:()=>{window.location.href="/blog/2023/latex/"}},{id:"post-java-collections",title:"Java Collections",description:"A brief reference for Java collections usecases.",section:"Posts",handler:()=>{window.location.href="/blog/2023/java-collection/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"posts-java-collections",title:"Java Collections",description:"A brief reference for Java collections usecases.",section:"Posts",handler:()=>{window.location.href="/blog/2023/java-collection/"}},{id:"posts-the-beauty-of-latex",title:"The Beauty of LaTeX",description:"When was the first time you had to use LaTeX? If you are like most people, it was probably suddenly forced upon you during your first math or CS class where you had to start writing proofs, with minimal guidance on how to get started. Unfortunately, this meant that while many people have good operational knowledge of LaTeX, there are still many small mistakes and best practices which are not followed, which are not corrected by TAs as they are either not severe enough to warrant a note, or perhaps even the TAs themselves are not aware of them.  In this post, we cover some common mistakes that are made by LaTeX practitioners (even in heavily cited papers), and how to address them.",section:"Posts",handler:()=>{window.location.href="/blog/2023/latex/"}},{id:"posts-deep-learning",title:"Deep Learning",description:"A curated list of resources for a comprehensive understanding of deep learning.  ",section:"Posts",handler:()=>{window.location.href="/blog/2024/deep-learning/"}},{id:"posts-dynamic-programming",title:"Dynamic Programming",description:"Setting up the foundations of Dynamic programming along with a generic framework to disintegrate the problem into components",section:"Posts",handler:()=>{window.location.href="/blog/2024/dynamic-prog/"}},{id:"posts-pythonic-templates",title:"Pythonic Templates",description:"A collection of code templates for common patterns in data structures and algorithms",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-template/"}},{id:"posts-dsa-templates-java",title:"DSA templates (Java)",description:"These are java code templates based on most common datastructures and algorithm problems.",section:"Posts",handler:()=>{window.location.href="/blog/2024/java-template/"}},{id:"posts-how-to-write-a-review-paper",title:"How to write a review paper?",description:"A walk through guide for structuring your research",section:"Posts",handler:()=>{window.location.href="/blog/2024/review-paper/"}},{id:"posts-research-my-two-cents",title:"Research: My two cents",description:"These are the nuggets of wisdom I found in the corners of internet while doing my own stint of research. Enjoy.",section:"Posts",handler:()=>{window.location.href="/blog/2024/research/"}},{id:"posts-reading",title:"Reading",description:"A personal reading list.",section:"Posts",handler:()=>{window.location.href="/blog/2024/library/"}},{id:"posts-sample-test",title:"Sample Test",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/sample/"}},{id:"projects-deepglobe-land-cover-classification",title:"DeepGlobe Land Cover Classification",description:"Classifying land coverage using remote sensing satelite data.",section:"Projects",handler:()=>{window.location.href="/projects/DeepGlobe/"}},{id:"projects-gitlet",title:"GitLet",description:"A version control with core functionalities of git, written in Java.",section:"Projects",handler:()=>{window.location.href="/projects/GitLet/"}},{id:"projects-blight-detection",title:"Blight Detection",description:"Assisting the farmers by potato blight grade detection.",section:"Projects",handler:()=>{window.location.href="/projects/blight/"}},{id:"projects-driver-drowsiness-detection-system",title:"Driver Drowsiness Detection System",description:"Determining the distracted or drowsiness state of drivers based on live facial features.",section:"Projects",handler:()=>{window.location.href="/projects/ddds/"}},{id:"projects-classifier-from-scratch",title:"Classifier from Scratch",description:"Designing a linear classifier from scratch.",section:"Projects",handler:()=>{window.location.href="/projects/lin-class/"}},{id:"projects-market-orderbook",title:"Market Orderbook",description:"Simulation",section:"Projects",handler:()=>{window.location.href="/projects/market/"}},{id:"projects-know-your-rice",title:"Know your Rice \ud83c\udf5a",description:"Rice classification using Deep learning.",section:"Projects",handler:()=>{window.location.href="/projects/rice/"}},{id:"projects-vinci",title:"Vinci",description:"A Java Genetic Algorithm implementation with a focus on ease of use and extensibility.",section:"Projects",handler:()=>{window.location.href="/projects/vinci/"}},{id:"summaries-deep-contextualized-word-representations-elmo",title:"Deep contextualized word representations (ELMo)",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-02-deep-contextualized-word-representations/"}},{id:"summaries-foundations-and-trends-in-multimodal-machine-learning-principles-challenges-and-open-questions",title:"Foundations and Trends in Multimodal Machine Learning: Principles,  Challenges, and Open Questions",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-02-foundations-and-trends-in-multimodal-machine-learning-principles--challenges-and-open-questions/"}},{id:"summaries-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding",title:"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-03-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding/"}},{id:"summaries-chain-of-thought-prompting-elicits-reasoning-in-large-language-models",title:"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-03-chain-of-thought-prompting-elicits-reasoning-in-large-language-models/"}},{id:"summaries-training-language-models-to-follow-instructions-with-human-feedback-instructgpt",title:"Training language models to follow instructions with human feedback (InstructGPT)",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-05-training-language-models-to-follow-instructions-with-human-feedback/"}},{id:"summaries-evaluating-large-language-models-trained-on-code-codex",title:"Evaluating Large Language Models Trained on Code (Codex)",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-06-evaluating-large-language-models-trained-on-code/"}},{id:"summaries-bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension",title:"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-09-bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension/"}},{id:"summaries-dense-passage-retrieval-for-open-domain-question-answering",title:"Dense Passage Retrieval for Open-Domain Question Answering",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-10-dense-passage-retrieval-for-open-domain-question-answering/"}},{id:"summaries-language-models-are-unsupervised-multitask-learners-gpt-2",title:"Language Models are Unsupervised Multitask Learners (GPT-2)",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-10-language-models-are-unsupervised-multitask-learners/"}},{id:"summaries-simple-synthetic-data-reduces-sycophancy-in-large-language-models",title:"Simple synthetic data reduces sycophancy in large language models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-10-simple-synthetic-data-reduces-sycophancy-in-large-language-models/"}},{id:"summaries-generative-agents-interactive-simulacra-of-human-behavior",title:"Generative Agents: Interactive Simulacra of Human Behavior",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-11-generative-agents-interactive-simulacra-of-human-behavior/"}},{id:"summaries-improving-language-understanding-by-generative-pre-training-gpt",title:"Improving Language Understanding by Generative Pre-Training (GPT)",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-11-improving-language-understanding-by-generative-pre-training/"}},{id:"summaries-metagpt-meta-programming-for-multi-agent-collaborative-framework",title:"MetaGPT: Meta Programming for Multi-Agent Collaborative Framework",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-11-metagpt-meta-programming-for-multi-agent-collaborative-framework/"}},{id:"summaries-transformers-in-speech-processing-a-survey",title:"Transformers in Speech Processing: A Survey",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-19-transformers-in-speech-processing-a-survey/"}},{id:"summaries-accurate-detection-of-wake-word-start-and-end-using-a-cnn",title:"Accurate Detection of Wake Word Start and End Using a CNN",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-22-accurate-detection-of-wake-word-start-and-end-using-a-cnn/"}},{id:"summaries-exploring-the-limits-of-transfer-learning-with-a-unified-text-to-text-transformer",title:"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-22-exploring-the-limits-of-transfer-learning-with-a-unified-text-to-text-transformer/"}},{id:"summaries-efficiently-modeling-long-sequences-with-structured-state-spaces",title:"Efficiently Modeling Long Sequences with Structured State Spaces",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-25-efficiently-modeling-long-sequences-with-structured-state-spaces/"}},{id:"summaries-a-watermark-for-large-language-models",title:"A Watermark for Large Language Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-27-a-watermark-for-large-language-models/"}},{id:"summaries-extracting-training-data-from-large-language-models",title:"Extracting Training Data from Large Language Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-29-extracting-training-data-from-large-language-models/"}},{id:"summaries-loss-landscapes-and-optimization-in-over-parameterized-non-linear-systems-and-neural-networks",title:"Loss Landscapes and Optimization in Over-Parameterized Non-Linear Systems and Neural Networks",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-09-04-loss-landscapes-and-optimization-in-over-parameterized-non-linear-systems-and-neural-networks/"}},{id:"summaries-gradient-descent-provably-optimizes-over-parameterized-neural-networks",title:"Gradient Descent Provably Optimizes Over-parameterized Neural Networks",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-09-07-gradient-descent-provably-optimizes-over-parameterized-neural-networks/"}},{id:"summaries-the-implicit-bias-of-gradient-descent-on-separable-data",title:"The Implicit Bias of Gradient Descent on Separable Data",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-09-09-the-implicit-bias-of-gradient-descent-on-separable-data/"}},{id:"summaries-understanding-deep-learning-requires-rethinking-generalization",title:"Understanding Deep Learning Requires Rethinking Generalization",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-09-10-understanding-deep-learning-requires-rethinking-generalization/"}},{id:"summaries-calibrate-before-use-improving-few-shot-performance-of-language-models",title:"Calibrate Before Use: Improving Few-Shot Performance of Language Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-09-23-calibrate-before-use-improving-few-shot-performance-of-language-models/"}},{id:"summaries-rethinking-the-role-of-demonstrations-what-makes-in-context-learning-work",title:"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-09-24-rethinking-the-role-of-demonstrations-what-makes-in-context-learning-work/"}},{id:"summaries-repository-level-prompt-generation-for-large-language-models-of-code",title:"Repository-Level Prompt Generation for Large Language Models of Code",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-09-26-repository-level-prompt-generation-for-large-language-models-of-code/"}},{id:"summaries-on-the-dangers-of-stochastic-parrots-can-language-models-be-too-big",title:"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-10-04-on-the-dangers-of-stochastic-parrots-can-language-models-be-too-big/"}},{id:"summaries-an-image-is-worth-one-word-personalizing-text-to-image-generation-using-textual-inversion",title:"An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-10-15-an-image-is-worth-one-word-personalizing-text-to-image-generation-using-textual-inversion/"}},{id:"summaries-instructpix2pix-learning-to-follow-image-editing-instructions",title:"InstructPix2Pix: Learning to Follow Image Editing Instructions",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-10-17-instructpix2pix-learning-to-follow-image-editing-instructions/"}},{id:"summaries-zero-shot-image-to-image-translation",title:"Zero-shot Image-to-Image Translation",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-10-22-zero-shot-image-to-image-translation/"}},{id:"summaries-universal-and-transferable-adversarial-attacks-on-aligned-language-models",title:"Universal and Transferable Adversarial Attacks on Aligned Language Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-10-31-universal-and-transferable-adversarial-attacks-on-aligned-language-models/"}},{id:"summaries-high-resolution-image-synthesis-with-latent-diffusion-models",title:"High-Resolution Image Synthesis with Latent Diffusion Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-11-03-high-resolution-image-synthesis-with-latent-diffusion-models/"}},{id:"summaries-large-language-models-for-software-engineering-survey-and-open-problems",title:"Large Language Models for Software Engineering: Survey and Open Problems",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-12-12-large-language-models-for-software-engineering-survey-and-open-problems/"}},{id:"summaries-matryoshka-representation-learning",title:"Matryoshka Representation Learning",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2024-02-19-matryoshka-representation-learning/"}},{id:"summaries-colbert-efficient-and-effective-passage-search-via-contextualized-late-interaction-over-bert",title:"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2024-02-22-colbert-efficient-and-effective-passage-search-via-contextualized-late-interaction-over-bert/"}},{id:"summaries-dspy-compiling-declarative-language-model-calls-into-self-improving-pipelines",title:"DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2024-03-15-dspy-compiling-declarative-language-model-calls-into-self-improving-pipelines/"}},{id:"summaries-scaling-laws-for-neural-language-models",title:"Scaling Laws for Neural Language Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2024-03-23-scaling-laws-for-neural-language-models/"}},{id:"summaries-training-compute-optimal-large-language-models",title:"Training Compute-Optimal Large Language Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2024-03-23-training-compute-optimal-large-language-models/"}},{id:"summaries-longrope-extending-llm-context-window-beyond-2-million-tokens",title:"LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2024-03-27-longrope-extending-llm-context-window-beyond-2-million-tokens/"}},{id:"work-wordless-ink-on-paper",title:"Wordless Ink on Paper",description:"",section:"Work",handler:()=>{window.location.href="/work/art/"}},{id:"work-machine-learning-research-internship-iit-delhi",title:"Machine Learning Research Internship @ IIT-Delhi",description:"",section:"Work",handler:()=>{window.location.href="/work/iitd/"}},{id:"work-rese",title:"Rese.",description:"",section:"Work",handler:()=>{window.location.href="/work/iitr/"}},{id:"work-research-statement",title:"Research Statement",description:"",section:"Work",handler:()=>{window.location.href="/work/research-statement/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%68%72%69%73%68%69%6B%65%73%68.%68%73%6B@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=ogGhORwAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/hrishikeshh","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/hrishikesh-singh","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> </body> </html>