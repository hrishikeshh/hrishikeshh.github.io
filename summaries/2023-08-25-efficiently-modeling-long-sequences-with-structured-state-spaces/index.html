<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Efficiently Modeling Long Sequences with Structured State Spaces | Hrishi </title> <meta name="author" content="Hrishikesh Singh"> <meta name="description" content="Collections of my thoughts, work and notes. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%90&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hrishikeshh.github.io/summaries/2023-08-25-efficiently-modeling-long-sequences-with-structured-state-spaces/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Hrishi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item active"> <a class="nav-link" href="/summaries/">ML Paper Summaries <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Notes </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div style="display:none"> $$ \newcommand{\bone}{\mathbf{1}} \newcommand{\bbeta}{\mathbf{\beta}} \newcommand{\bdelta}{\mathbf{\delta}} \newcommand{\bepsilon}{\mathbf{\epsilon}} \newcommand{\blambda}{\mathbf{\lambda}} \newcommand{\bomega}{\mathbf{\omega}} \newcommand{\bpi}{\mathbf{\pi}} \newcommand{\bphi}{\mathbf{\phi}} \newcommand{\bvphi}{\mathbf{\varphi}} \newcommand{\bpsi}{\mathbf{\psi}} \newcommand{\bsigma}{\mathbf{\sigma}} \newcommand{\btheta}{\mathbf{\theta}} \newcommand{\btau}{\mathbf{\tau}} \newcommand{\ba}{\mathbf{a}} \newcommand{\bb}{\mathbf{b}} \newcommand{\bc}{\mathbf{c}} \newcommand{\bd}{\mathbf{d}} \newcommand{\be}{\mathbf{e}} \newcommand{\boldf}{\mathbf{f}} \newcommand{\bg}{\mathbf{g}} \newcommand{\bh}{\mathbf{h}} \newcommand{\bi}{\mathbf{i}} \newcommand{\bj}{\mathbf{j}} \newcommand{\bk}{\mathbf{k}} \newcommand{\bell}{\mathbf{\ell}} \newcommand{\bm}{\mathbf{m}} \newcommand{\bn}{\mathbf{n}} \newcommand{\bo}{\mathbf{o}} \newcommand{\bp}{\mathbf{p}} \newcommand{\bq}{\mathbf{q}} \newcommand{\br}{\mathbf{r}} \newcommand{\bs}{\mathbf{s}} \newcommand{\bt}{\mathbf{t}} \newcommand{\bu}{\mathbf{u}} \newcommand{\bv}{\mathbf{v}} \newcommand{\bw}{\mathbf{w}} \newcommand{\bx}{\mathbf{x}} \newcommand{\by}{\mathbf{y}} \newcommand{\bz}{\mathbf{z}} \newcommand{\bA}{\mathbf{A}} \newcommand{\bB}{\mathbf{B}} \newcommand{\bC}{\mathbf{C}} \newcommand{\bD}{\mathbf{D}} \newcommand{\bE}{\mathbf{E}} \newcommand{\bF}{\mathbf{F}} \newcommand{\bG}{\mathbf{G}} \newcommand{\bH}{\mathbf{H}} \newcommand{\bI}{\mathbf{I}} \newcommand{\bJ}{\mathbf{J}} \newcommand{\bK}{\mathbf{K}} \newcommand{\bL}{\mathbf{L}} \newcommand{\bM}{\mathbf{M}} \newcommand{\bN}{\mathbf{N}} \newcommand{\bP}{\mathbf{P}} \newcommand{\bQ}{\mathbf{Q}} \newcommand{\bR}{\mathbf{R}} \newcommand{\bS}{\mathbf{S}} \newcommand{\bT}{\mathbf{T}} \newcommand{\bU}{\mathbf{U}} \newcommand{\bV}{\mathbf{V}} \newcommand{\bW}{\mathbf{W}} \newcommand{\bX}{\mathbf{X}} \newcommand{\bY}{\mathbf{Y}} \newcommand{\bZ}{\mathbf{Z}} \newcommand{\calA}{\mathcal{A}} \newcommand{\calB}{\mathcal{B}} \newcommand{\calC}{\mathcal{C}} \newcommand{\calD}{\mathcal{D}} \newcommand{\calE}{\mathcal{E}} \newcommand{\calF}{\mathcal{F}} \newcommand{\calG}{\mathcal{G}} \newcommand{\calH}{\mathcal{H}} \newcommand{\calI}{\mathcal{I}} \newcommand{\calJ}{\mathcal{J}} \newcommand{\calK}{\mathcal{K}} \newcommand{\calL}{\mathcal{L}} \newcommand{\calM}{\mathcal{M}} \newcommand{\calN}{\mathcal{N}} \newcommand{\calO}{\mathcal{O}} \newcommand{\calP}{\mathcal{P}} \newcommand{\calQ}{\mathcal{Q}} \newcommand{\calR}{\mathcal{R}} \newcommand{\calS}{\mathcal{S}} \newcommand{\calT}{\mathcal{T}} \newcommand{\calU}{\mathcal{U}} \newcommand{\calV}{\mathcal{V}} \newcommand{\calW}{\mathcal{W}} \newcommand{\calX}{\mathcal{X}} \newcommand{\calY}{\mathcal{Y}} \newcommand{\calZ}{\mathcal{Z}} \newcommand{\R}{\mathbb{R}} \newcommand{\C}{\mathbb{C}} \newcommand{\N}{\mathbb{N}} \newcommand{\Z}{\mathbb{Z}} \newcommand{\F}{\mathbb{F}} \newcommand{\Q}{\mathbb{Q}} \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min} \newcommand{\nnz}[1]{\mbox{nnz}(#1)} \newcommand{\dotprod}[2]{\langle #1, #2 \rangle} \newcommand{\ignore}[1]{} \let\Pr\relax \DeclareMathOperator*{\Pr}{\mathbf{Pr}} \newcommand{\E}{\mathbb{E}} \DeclareMathOperator*{\Ex}{\mathbf{E}} \DeclareMathOperator*{\Var}{\mathbf{Var}} \DeclareMathOperator*{\Cov}{\mathbf{Cov}} \DeclareMathOperator*{\stddev}{\mathbf{stddev}} \DeclareMathOperator*{\avg}{avg} \DeclareMathOperator{\poly}{poly} \DeclareMathOperator{\polylog}{polylog} \DeclareMathOperator{\size}{size} \DeclareMathOperator{\sgn}{sgn} \DeclareMathOperator{\dist}{dist} \DeclareMathOperator{\vol}{vol} \DeclareMathOperator{\spn}{span} \DeclareMathOperator{\supp}{supp} \DeclareMathOperator{\tr}{tr} \DeclareMathOperator{\Tr}{Tr} \DeclareMathOperator{\codim}{codim} \DeclareMathOperator{\diag}{diag} \newcommand{\PTIME}{\mathsf{P}} \newcommand{\LOGSPACE}{\mathsf{L}} \newcommand{\ZPP}{\mathsf{ZPP}} \newcommand{\RP}{\mathsf{RP}} \newcommand{\BPP}{\mathsf{BPP}} \newcommand{\P}{\mathsf{P}} \newcommand{\NP}{\mathsf{NP}} \newcommand{\TC}{\mathsf{TC}} \newcommand{\AC}{\mathsf{AC}} \newcommand{\SC}{\mathsf{SC}} \newcommand{\SZK}{\mathsf{SZK}} \newcommand{\AM}{\mathsf{AM}} \newcommand{\IP}{\mathsf{IP}} \newcommand{\PSPACE}{\mathsf{PSPACE}} \newcommand{\EXP}{\mathsf{EXP}} \newcommand{\MIP}{\mathsf{MIP}} \newcommand{\NEXP}{\mathsf{NEXP}} \newcommand{\BQP}{\mathsf{BQP}} \newcommand{\distP}{\mathsf{dist\textbf{P}}} \newcommand{\distNP}{\mathsf{dist\textbf{NP}}} \newcommand{\eps}{\epsilon} \newcommand{\lam}{\lambda} \newcommand{\dleta}{\delta} \newcommand{\simga}{\sigma} \newcommand{\vphi}{\varphi} \newcommand{\la}{\langle} \newcommand{\ra}{\rangle} \newcommand{\wt}[1]{\widetilde{#1}} \newcommand{\wh}[1]{\widehat{#1}} \newcommand{\ol}[1]{\overline{#1}} \newcommand{\ul}[1]{\underline{#1}} \newcommand{\ot}{\otimes} \newcommand{\zo}{\{0,1\}} \newcommand{\co}{:} %\newcommand{\co}{\colon} \newcommand{\bdry}{\partial} \newcommand{\grad}{\nabla} \newcommand{\transp}{^\intercal} \newcommand{\inv}{^{-1}} \newcommand{\symmdiff}{\triangle} \newcommand{\symdiff}{\symmdiff} \newcommand{\half}{\tfrac{1}{2}} \newcommand{\bbone}{\mathbbm 1} \newcommand{\Id}{\bbone} \newcommand{\SAT}{\mathsf{SAT}} \newcommand{\bcalG}{\boldsymbol{\calG}} \newcommand{\calbG}{\bcalG} \newcommand{\bcalX}{\boldsymbol{\calX}} \newcommand{\calbX}{\bcalX} \newcommand{\bcalY}{\boldsymbol{\calY}} \newcommand{\calbY}{\bcalY} \newcommand{\bcalZ}{\boldsymbol{\calZ}} \newcommand{\calbZ}{\bcalZ} $$ </div> <div class="publications"> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row no-count"> <div id="2111.00396v3" class="col-sm-12"> <div class="title"><h2> <a href="http://arxiv.org/abs/2111.00396v3" rel="external nofollow noopener" target="_blank"> Efficiently Modeling Long Sequences with Structured State Spaces </a> </h2></div> <div class="author"> Albert Gu, Karan Goel, and Christopher Ré</div> <div class="periodical"> <em></em> Oct 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2111.00396v3" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2111.00396v3.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <h3>Paper Abstract</h3> <div class="abstract"> <p>A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of 10000 or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \(x’(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \), and showed that for appropriate choices of the state matrix \(A \), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \(A \)with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation 60\times faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">2111.00396v3</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gu, Albert and Goel, Karan and Ré, Christopher}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Efficiently Modeling Long Sequences with Structured State Spaces}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2111.00396v3}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.LG}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://arxiv.org/abs/2111.00396v3}</span><span class="p">,</span>
  <span class="na">file</span> <span class="p">=</span> <span class="s">{2111.00396v3.pdf}</span><span class="p">,</span>
  <span class="na">eprintnover</span> <span class="p">=</span> <span class="s">{2111.00396}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <div class="post"> <article class="post-content"> <div id="markdown-content"> <h3 id="three-important-things">Three Important Things</h3> <h4 id="1-the-problem-with-discrete-time-state-sequence-models-ssms">1. The Problem with Discrete-time State Sequence Models (SSMs)</h4> <p>The paper investigates improving upon the state-of-the-art performance on sequential tasks that involve very long sequences. The current state-of-the-art is based on Transformers models, but these suffer from severe computational limitations such as a quadratic cost on computing cross-attention based on sequence length.</p> <p>One possible approach to doing this is known as the State Space Model (SSM). This works as follows:</p> <ol> <li>There are four matrices to be learned: \(\bA, \bB, \bC, \bD\).</li> <li>Let \(u(t)\) be a 1D input signal at time \(t\).</li> <li>We model the output signal using the following equation: \(\begin{align} x'(t) &amp;= \bA x(t) + \bB u(t) \\ y(t) &amp;= \bC x(t) + \bD u(t) \\ \end{align}\)</li> </ol> <p>Note that \(x'(t)\) is written as such to denote it as the new value of \(x\), which is constantly being updated every time step. You can think of \(x(t)\) as a form of a hidden state that is updated every timestep (like a continuous analog of RNNs), in response to the continuous input \(u(t)\).</p> <p>Since we work with computers in practice, we need to discretize the updates with step sizes \(\Delta\). This can be achieved using a classical technique in digital signal processing known as the <a href="https://en.wikipedia.org/wiki/Bilinear_transform" rel="external nofollow noopener" target="_blank">bilinear transform</a>, which results in the following form at each timestep \(k\):</p> \[\newcommand{\oA}{\overline{\bA}} \newcommand{\oB}{\overline{\bB}} \newcommand{\oC}{\overline{\bC}} \newcommand{\oK}{\overline{\bK}} \begin{align} \oA &amp;= (\bI - \Delta/2 \cdot \bA)^{-1} (\bI + \Delta/2 \cdot \bA) \\ \oB &amp;= (\bI - \Delta/2 \cdot \bA)^{-1} \Delta \bB \\ \oC &amp;= \bC \\ x_k &amp;= \oA x_{k-1} + \oB u_k \\ y_k &amp;= \oC x_{k} \\ \end{align}\] <p>However, this still suffers from the limitation that the recurrent updates are sequentially applied, resulting in runtime as long as the sequence length which is not parallelizable.</p> <p>Instead, the authors show that when you unroll the recurrent steps, notice you get something like the following:</p> \[\begin{align} x_0 &amp; = \oB u_0 \\ y_0 &amp; = \overline{\bC \bB} u_0 \\ x_1 &amp; = \overline{\bA \bB} u_0 + \overline{\bB} u_1 \\ y_1 &amp; = \overline{\bC \bA \bB} u_0 + \overline{\bC \bB} u_1 \\ x_2 &amp; = \oA^2 \oB u_0 + \overline{\bA \bB} u_1 + \oB u_2 \\ y_2 &amp; = \oC \oA^2 \oB u_0 + \overline{\bC \bA \bB} u_1 + \overline{\bC \bB} u_2 \\ &amp; \vdots \\ \end{align}\] <p>This looks like the summation of a <a href="https://en.wikipedia.org/wiki/Convolution#Discrete_convolution" rel="external nofollow noopener" target="_blank">discrete convolution</a>, recall that a discrete convolution has the following form:</p> \[(f * g)[n] = \sum_{m=- \infty}^{\infty} f[m] g[n-m]\] <p>Indeed, letting \(L\) be the discretized sequence length of \(y\), we can express this with a single convolutional kernel \(\oK\):</p> \[\begin{align} y &amp; = \oK * u \\ \oK \in \mathbb{R}^L &amp; \coloneqq (\overline{\bC \bB}, \overline{\bC \bA \bB}, \cdots, \overline{\bC \bA}^{L-1} \oB) \end{align}\] <p>If we could compute this \(\oK\) efficiently, then we are done, but alas this is not the case.</p> <h4 id="2-hippo-matrix">2. HiPPO Matrix</h4> <p>The HiPPO matrix was introduced in their prior paper <a href="https://arxiv.org/abs/2008.07669" rel="external nofollow noopener" target="_blank">HiPPO: Recurrent Memory with Optimal Polynomial Projections</a>, but is worth mentioning here as well due to its importance in subsequent analysis.</p> <p>The main idea is that instead of letting \(\bA\) just be anything, training performs a lot better if \(\bA\) is fixed to be the HiPPO matrix, defined as follows:</p> \[\textbf{HiPPO Matrix} \qquad \bA_{nk} = - \begin{cases} (2n + 1)^{1/2} (2k + 1)^{1/2} &amp; \text{if $n &gt; k$,} \\ n + 1 &amp; \text{if $n = k$,} \\ 0 &amp; \text{if $n &lt; k$.} \\ \end{cases}\] <h4 id="3-structured-state-space-sequence-model-s4">3. Structured State Space sequence model (S4)</h4> <p>To compute \(\oK\) efficiently, the authors introduced the Structured State Space sequence model (S4), which is the main contribution of the paper. It is also worth mentioning that they</p> <p>The main bottleneck of computing the kernel \(\oK\) is the need to iteratively compute \(\oA^k\). One possible might be to consider the conjugation of \(\bA\) by some matrix \(\bV\), to obtain an equivalence relation</p> \[(\bA, \bB, \bC) \sim (\bV^{-1} \bA \bV, \bV^{-1} \bB, \bC \bV),\] <p>with the benefit that \(\bV^{-1} \bA \bV\) is now diagonalizable, which allows us to compute \((\bV^{-1} \bA \bV)^k\) quickly.</p> <p>However, this does not work in practice due to numerical stability issues, since the diagonalization does not have to be well-conditioned (i.e a large ratio between its smallest and largest eigenvalues).</p> <p>To resolve this, they show that the following steps (in the figure below) can be applied to any matrix that can be decomposed as Normal Plus Low-Rank (NPLR). A NPLR representation means that it can be expressed as the sum of a normal and low-rank matrix. A matrix is <a href="https://en.wikipedia.org/wiki/Normal_matrix" rel="external nofollow noopener" target="_blank">normal</a> if it commutes with its conjugate transpose, i.e</p> \[\bA^* \bA = \bA \bA^*.\] <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/summaries/s4-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/summaries/s4-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/summaries/s4-1400.webp"></source> <img src="/assets/img/summaries/s4.webp" class="z-depth-1 center" width="600px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Understanding the specifics of each of these steps is currently above my pay grade, but I will update this page again in the event that I receive enlightenment someday.</p> <p>The authors then proved that all HiPPO matrices have a NPLR representation, and concludes with a theorem that states that \(\oK\) can be computed using only \(\tilde{O}(N + L)\) operations and \(O(N + L)\) space.</p> <p>They then showed that this setup results in state-of-the-art performance on many tasks with long-range dependencies, outperforming Transformers and its variants.</p> <h3 id="most-glaring-deficiency">Most Glaring Deficiency</h3> <p>In many ways, the S4 model feels reminiscent of a RNN, except it uses a HiPPO matrix for updating its hidden state, which gives rise to opportunities for speedups which is the main focus of this paper.</p> <p>In this manner, would a traditional RNN approach have performed just as well if the matrix for updating the hidden state was also the HiPPO matrix? This was a question that could have been answered.</p> <p>It was unclear to me intuitively how the conceptually simple S4 model is somehow capable of capturing long-range dependencies, which plagues regular RNNs. Admittedly this may have been addressed more in-depth in the previous HiPPO paper, but it would make the paper even better if they included some hypotheses on why it works well.</p> <h3 id="conclusions-for-future-work">Conclusions for Future Work</h3> <p>This paper showed that state space sequence models can be a viable technique for capturing long-range dependencies in sequential data, by employing a variety of tricks. This technique could inspire future applications that require such capabilities.</p> </div> </article> <p class="post-meta">Written 2023</p> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Hrishikesh Singh. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script> </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let theme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===theme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=document.querySelector(".navbar-collapse");e.classList.contains("show")&&e.classList.remove("show"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"Publications",description:"Papers under review/submission and pre-prints are available upon request.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-ml-paper-summaries",title:"ML Paper Summaries",description:"",section:"Navigation",handler:()=>{window.location.href="/summaries/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-notes",title:"Notes",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-sample-test",title:"Sample Test",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/sample/"}},{id:"post-reading",title:"Reading",description:"A personal reading list.",section:"Posts",handler:()=>{window.location.href="/blog/2024/library/"}},{id:"post-research-my-two-cents",title:"Research: My two cents",description:"These are the nuggets of wisdom I found in the corners of internet while doing my own stint of research. Enjoy.",section:"Posts",handler:()=>{window.location.href="/blog/2024/research/"}},{id:"post-how-to-write-a-review-paper",title:"How to write a review paper?",description:"A walk through guide for structuring your research",section:"Posts",handler:()=>{window.location.href="/blog/2024/review-paper/"}},{id:"post-dsa-templates-java",title:"DSA templates (Java)",description:"These are java code templates based on most common datastructures and algorithm problems.",section:"Posts",handler:()=>{window.location.href="/blog/2024/java-template/"}},{id:"post-pythonic-templates",title:"Pythonic Templates",description:"A collection of code templates for common patterns in data structures and algorithms",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-template/"}},{id:"post-dynamic-programming",title:"Dynamic Programming",description:"Setting up the foundations of Dynamic programming along with a generic framework to disintegrate the problem into components",section:"Posts",handler:()=>{window.location.href="/blog/2024/dynamic-prog/"}},{id:"post-deep-learning",title:"Deep Learning",description:"A curated list of resources for a comprehensive understanding of deep learning.  ",section:"Posts",handler:()=>{window.location.href="/blog/2024/deep-learning/"}},{id:"post-the-beauty-of-latex",title:"The Beauty of LaTeX",description:"When was the first time you had to use LaTeX? If you are like most people, it was probably suddenly forced upon you during your first math or CS class where you had to start writing proofs, with minimal guidance on how to get started. Unfortunately, this meant that while many people have good operational knowledge of LaTeX, there are still many small mistakes and best practices which are not followed, which are not corrected by TAs as they are either not severe enough to warrant a note, or perhaps even the TAs themselves are not aware of them.  In this post, we cover some common mistakes that are made by LaTeX practitioners (even in heavily cited papers), and how to address them.",section:"Posts",handler:()=>{window.location.href="/blog/2023/latex/"}},{id:"post-java-collections",title:"Java Collections",description:"A brief reference for Java collections usecases.",section:"Posts",handler:()=>{window.location.href="/blog/2023/java-collection/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"posts-java-collections",title:"Java Collections",description:"A brief reference for Java collections usecases.",section:"Posts",handler:()=>{window.location.href="/blog/2023/java-collection/"}},{id:"posts-the-beauty-of-latex",title:"The Beauty of LaTeX",description:"When was the first time you had to use LaTeX? If you are like most people, it was probably suddenly forced upon you during your first math or CS class where you had to start writing proofs, with minimal guidance on how to get started. Unfortunately, this meant that while many people have good operational knowledge of LaTeX, there are still many small mistakes and best practices which are not followed, which are not corrected by TAs as they are either not severe enough to warrant a note, or perhaps even the TAs themselves are not aware of them.  In this post, we cover some common mistakes that are made by LaTeX practitioners (even in heavily cited papers), and how to address them.",section:"Posts",handler:()=>{window.location.href="/blog/2023/latex/"}},{id:"posts-deep-learning",title:"Deep Learning",description:"A curated list of resources for a comprehensive understanding of deep learning.  ",section:"Posts",handler:()=>{window.location.href="/blog/2024/deep-learning/"}},{id:"posts-dynamic-programming",title:"Dynamic Programming",description:"Setting up the foundations of Dynamic programming along with a generic framework to disintegrate the problem into components",section:"Posts",handler:()=>{window.location.href="/blog/2024/dynamic-prog/"}},{id:"posts-pythonic-templates",title:"Pythonic Templates",description:"A collection of code templates for common patterns in data structures and algorithms",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-template/"}},{id:"posts-dsa-templates-java",title:"DSA templates (Java)",description:"These are java code templates based on most common datastructures and algorithm problems.",section:"Posts",handler:()=>{window.location.href="/blog/2024/java-template/"}},{id:"posts-how-to-write-a-review-paper",title:"How to write a review paper?",description:"A walk through guide for structuring your research",section:"Posts",handler:()=>{window.location.href="/blog/2024/review-paper/"}},{id:"posts-research-my-two-cents",title:"Research: My two cents",description:"These are the nuggets of wisdom I found in the corners of internet while doing my own stint of research. Enjoy.",section:"Posts",handler:()=>{window.location.href="/blog/2024/research/"}},{id:"posts-reading",title:"Reading",description:"A personal reading list.",section:"Posts",handler:()=>{window.location.href="/blog/2024/library/"}},{id:"posts-sample-test",title:"Sample Test",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/sample/"}},{id:"projects-deepglobe-land-cover-classification",title:"DeepGlobe Land Cover Classification",description:"Classifying land coverage using remote sensing satelite data.",section:"Projects",handler:()=>{window.location.href="/projects/DeepGlobe/"}},{id:"projects-gitlet",title:"GitLet",description:"A version control with core functionalities of git, written in Java.",section:"Projects",handler:()=>{window.location.href="/projects/GitLet/"}},{id:"projects-blight-detection",title:"Blight Detection",description:"Assisting the farmers by potato blight grade detection.",section:"Projects",handler:()=>{window.location.href="/projects/blight/"}},{id:"projects-driver-drowsiness-detection-system",title:"Driver Drowsiness Detection System",description:"Determining the distracted or drowsiness state of drivers based on live facial features.",section:"Projects",handler:()=>{window.location.href="/projects/ddds/"}},{id:"projects-classifier-from-scratch",title:"Classifier from Scratch",description:"Designing a linear classifier from scratch.",section:"Projects",handler:()=>{window.location.href="/projects/lin-class/"}},{id:"projects-market-orderbook",title:"Market Orderbook",description:"Simulation",section:"Projects",handler:()=>{window.location.href="/projects/market/"}},{id:"projects-know-your-rice",title:"Know your Rice \ud83c\udf5a",description:"Rice classification using Deep learning.",section:"Projects",handler:()=>{window.location.href="/projects/rice/"}},{id:"projects-vinci",title:"Vinci",description:"A Java Genetic Algorithm implementation with a focus on ease of use and extensibility.",section:"Projects",handler:()=>{window.location.href="/projects/vinci/"}},{id:"summaries-deep-contextualized-word-representations-elmo",title:"Deep contextualized word representations (ELMo)",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-02-deep-contextualized-word-representations/"}},{id:"summaries-foundations-and-trends-in-multimodal-machine-learning-principles-challenges-and-open-questions",title:"Foundations and Trends in Multimodal Machine Learning: Principles,  Challenges, and Open Questions",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-02-foundations-and-trends-in-multimodal-machine-learning-principles--challenges-and-open-questions/"}},{id:"summaries-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding",title:"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-03-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding/"}},{id:"summaries-chain-of-thought-prompting-elicits-reasoning-in-large-language-models",title:"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-03-chain-of-thought-prompting-elicits-reasoning-in-large-language-models/"}},{id:"summaries-training-language-models-to-follow-instructions-with-human-feedback-instructgpt",title:"Training language models to follow instructions with human feedback (InstructGPT)",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-05-training-language-models-to-follow-instructions-with-human-feedback/"}},{id:"summaries-evaluating-large-language-models-trained-on-code-codex",title:"Evaluating Large Language Models Trained on Code (Codex)",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-06-evaluating-large-language-models-trained-on-code/"}},{id:"summaries-bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension",title:"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-09-bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension/"}},{id:"summaries-dense-passage-retrieval-for-open-domain-question-answering",title:"Dense Passage Retrieval for Open-Domain Question Answering",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-10-dense-passage-retrieval-for-open-domain-question-answering/"}},{id:"summaries-language-models-are-unsupervised-multitask-learners-gpt-2",title:"Language Models are Unsupervised Multitask Learners (GPT-2)",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-10-language-models-are-unsupervised-multitask-learners/"}},{id:"summaries-simple-synthetic-data-reduces-sycophancy-in-large-language-models",title:"Simple synthetic data reduces sycophancy in large language models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-10-simple-synthetic-data-reduces-sycophancy-in-large-language-models/"}},{id:"summaries-generative-agents-interactive-simulacra-of-human-behavior",title:"Generative Agents: Interactive Simulacra of Human Behavior",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-11-generative-agents-interactive-simulacra-of-human-behavior/"}},{id:"summaries-improving-language-understanding-by-generative-pre-training-gpt",title:"Improving Language Understanding by Generative Pre-Training (GPT)",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-11-improving-language-understanding-by-generative-pre-training/"}},{id:"summaries-metagpt-meta-programming-for-multi-agent-collaborative-framework",title:"MetaGPT: Meta Programming for Multi-Agent Collaborative Framework",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-11-metagpt-meta-programming-for-multi-agent-collaborative-framework/"}},{id:"summaries-transformers-in-speech-processing-a-survey",title:"Transformers in Speech Processing: A Survey",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-19-transformers-in-speech-processing-a-survey/"}},{id:"summaries-accurate-detection-of-wake-word-start-and-end-using-a-cnn",title:"Accurate Detection of Wake Word Start and End Using a CNN",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-22-accurate-detection-of-wake-word-start-and-end-using-a-cnn/"}},{id:"summaries-exploring-the-limits-of-transfer-learning-with-a-unified-text-to-text-transformer",title:"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-22-exploring-the-limits-of-transfer-learning-with-a-unified-text-to-text-transformer/"}},{id:"summaries-efficiently-modeling-long-sequences-with-structured-state-spaces",title:"Efficiently Modeling Long Sequences with Structured State Spaces",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-25-efficiently-modeling-long-sequences-with-structured-state-spaces/"}},{id:"summaries-a-watermark-for-large-language-models",title:"A Watermark for Large Language Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-27-a-watermark-for-large-language-models/"}},{id:"summaries-extracting-training-data-from-large-language-models",title:"Extracting Training Data from Large Language Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-29-extracting-training-data-from-large-language-models/"}},{id:"summaries-loss-landscapes-and-optimization-in-over-parameterized-non-linear-systems-and-neural-networks",title:"Loss Landscapes and Optimization in Over-Parameterized Non-Linear Systems and Neural Networks",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-09-04-loss-landscapes-and-optimization-in-over-parameterized-non-linear-systems-and-neural-networks/"}},{id:"summaries-gradient-descent-provably-optimizes-over-parameterized-neural-networks",title:"Gradient Descent Provably Optimizes Over-parameterized Neural Networks",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-09-07-gradient-descent-provably-optimizes-over-parameterized-neural-networks/"}},{id:"summaries-the-implicit-bias-of-gradient-descent-on-separable-data",title:"The Implicit Bias of Gradient Descent on Separable Data",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-09-09-the-implicit-bias-of-gradient-descent-on-separable-data/"}},{id:"summaries-understanding-deep-learning-requires-rethinking-generalization",title:"Understanding Deep Learning Requires Rethinking Generalization",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-09-10-understanding-deep-learning-requires-rethinking-generalization/"}},{id:"summaries-calibrate-before-use-improving-few-shot-performance-of-language-models",title:"Calibrate Before Use: Improving Few-Shot Performance of Language Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-09-23-calibrate-before-use-improving-few-shot-performance-of-language-models/"}},{id:"summaries-rethinking-the-role-of-demonstrations-what-makes-in-context-learning-work",title:"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-09-24-rethinking-the-role-of-demonstrations-what-makes-in-context-learning-work/"}},{id:"summaries-repository-level-prompt-generation-for-large-language-models-of-code",title:"Repository-Level Prompt Generation for Large Language Models of Code",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-09-26-repository-level-prompt-generation-for-large-language-models-of-code/"}},{id:"summaries-on-the-dangers-of-stochastic-parrots-can-language-models-be-too-big",title:"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-10-04-on-the-dangers-of-stochastic-parrots-can-language-models-be-too-big/"}},{id:"summaries-an-image-is-worth-one-word-personalizing-text-to-image-generation-using-textual-inversion",title:"An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-10-15-an-image-is-worth-one-word-personalizing-text-to-image-generation-using-textual-inversion/"}},{id:"summaries-instructpix2pix-learning-to-follow-image-editing-instructions",title:"InstructPix2Pix: Learning to Follow Image Editing Instructions",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-10-17-instructpix2pix-learning-to-follow-image-editing-instructions/"}},{id:"summaries-zero-shot-image-to-image-translation",title:"Zero-shot Image-to-Image Translation",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-10-22-zero-shot-image-to-image-translation/"}},{id:"summaries-universal-and-transferable-adversarial-attacks-on-aligned-language-models",title:"Universal and Transferable Adversarial Attacks on Aligned Language Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-10-31-universal-and-transferable-adversarial-attacks-on-aligned-language-models/"}},{id:"summaries-high-resolution-image-synthesis-with-latent-diffusion-models",title:"High-Resolution Image Synthesis with Latent Diffusion Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-11-03-high-resolution-image-synthesis-with-latent-diffusion-models/"}},{id:"summaries-large-language-models-for-software-engineering-survey-and-open-problems",title:"Large Language Models for Software Engineering: Survey and Open Problems",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-12-12-large-language-models-for-software-engineering-survey-and-open-problems/"}},{id:"summaries-matryoshka-representation-learning",title:"Matryoshka Representation Learning",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2024-02-19-matryoshka-representation-learning/"}},{id:"summaries-colbert-efficient-and-effective-passage-search-via-contextualized-late-interaction-over-bert",title:"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2024-02-22-colbert-efficient-and-effective-passage-search-via-contextualized-late-interaction-over-bert/"}},{id:"summaries-dspy-compiling-declarative-language-model-calls-into-self-improving-pipelines",title:"DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2024-03-15-dspy-compiling-declarative-language-model-calls-into-self-improving-pipelines/"}},{id:"summaries-scaling-laws-for-neural-language-models",title:"Scaling Laws for Neural Language Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2024-03-23-scaling-laws-for-neural-language-models/"}},{id:"summaries-training-compute-optimal-large-language-models",title:"Training Compute-Optimal Large Language Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2024-03-23-training-compute-optimal-large-language-models/"}},{id:"summaries-longrope-extending-llm-context-window-beyond-2-million-tokens",title:"LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2024-03-27-longrope-extending-llm-context-window-beyond-2-million-tokens/"}},{id:"work-wordless-ink-on-paper",title:"Wordless Ink on Paper",description:"",section:"Work",handler:()=>{window.location.href="/work/art/"}},{id:"work-machine-learning-research-internship-iit-delhi",title:"Machine Learning Research Internship @ IIT-Delhi",description:"",section:"Work",handler:()=>{window.location.href="/work/iitd/"}},{id:"work-rese",title:"Rese.",description:"",section:"Work",handler:()=>{window.location.href="/work/iitr/"}},{id:"work-research-statement",title:"Research Statement",description:"",section:"Work",handler:()=>{window.location.href="/work/research-statement/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%68%72%69%73%68%69%6B%65%73%68.%68%73%6B@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=ogGhORwAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/hrishikeshh","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/hrishikesh-singh","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> </body> </html>