<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Foundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions | Hrishi </title> <meta name="author" content="Hrishikesh Singh"> <meta name="description" content="Collections of my thoughts, work and notes. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%90&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hrishikeshh.github.io/summaries/2023-08-02-foundations-and-trends-in-multimodal-machine-learning-principles--challenges-and-open-questions/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Hrishi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item active"> <a class="nav-link" href="/summaries/">ML Paper Summaries <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Notes </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div style="display:none"> $$ \newcommand{\bone}{\mathbf{1}} \newcommand{\bbeta}{\mathbf{\beta}} \newcommand{\bdelta}{\mathbf{\delta}} \newcommand{\bepsilon}{\mathbf{\epsilon}} \newcommand{\blambda}{\mathbf{\lambda}} \newcommand{\bomega}{\mathbf{\omega}} \newcommand{\bpi}{\mathbf{\pi}} \newcommand{\bphi}{\mathbf{\phi}} \newcommand{\bvphi}{\mathbf{\varphi}} \newcommand{\bpsi}{\mathbf{\psi}} \newcommand{\bsigma}{\mathbf{\sigma}} \newcommand{\btheta}{\mathbf{\theta}} \newcommand{\btau}{\mathbf{\tau}} \newcommand{\ba}{\mathbf{a}} \newcommand{\bb}{\mathbf{b}} \newcommand{\bc}{\mathbf{c}} \newcommand{\bd}{\mathbf{d}} \newcommand{\be}{\mathbf{e}} \newcommand{\boldf}{\mathbf{f}} \newcommand{\bg}{\mathbf{g}} \newcommand{\bh}{\mathbf{h}} \newcommand{\bi}{\mathbf{i}} \newcommand{\bj}{\mathbf{j}} \newcommand{\bk}{\mathbf{k}} \newcommand{\bell}{\mathbf{\ell}} \newcommand{\bm}{\mathbf{m}} \newcommand{\bn}{\mathbf{n}} \newcommand{\bo}{\mathbf{o}} \newcommand{\bp}{\mathbf{p}} \newcommand{\bq}{\mathbf{q}} \newcommand{\br}{\mathbf{r}} \newcommand{\bs}{\mathbf{s}} \newcommand{\bt}{\mathbf{t}} \newcommand{\bu}{\mathbf{u}} \newcommand{\bv}{\mathbf{v}} \newcommand{\bw}{\mathbf{w}} \newcommand{\bx}{\mathbf{x}} \newcommand{\by}{\mathbf{y}} \newcommand{\bz}{\mathbf{z}} \newcommand{\bA}{\mathbf{A}} \newcommand{\bB}{\mathbf{B}} \newcommand{\bC}{\mathbf{C}} \newcommand{\bD}{\mathbf{D}} \newcommand{\bE}{\mathbf{E}} \newcommand{\bF}{\mathbf{F}} \newcommand{\bG}{\mathbf{G}} \newcommand{\bH}{\mathbf{H}} \newcommand{\bI}{\mathbf{I}} \newcommand{\bJ}{\mathbf{J}} \newcommand{\bK}{\mathbf{K}} \newcommand{\bL}{\mathbf{L}} \newcommand{\bM}{\mathbf{M}} \newcommand{\bN}{\mathbf{N}} \newcommand{\bP}{\mathbf{P}} \newcommand{\bQ}{\mathbf{Q}} \newcommand{\bR}{\mathbf{R}} \newcommand{\bS}{\mathbf{S}} \newcommand{\bT}{\mathbf{T}} \newcommand{\bU}{\mathbf{U}} \newcommand{\bV}{\mathbf{V}} \newcommand{\bW}{\mathbf{W}} \newcommand{\bX}{\mathbf{X}} \newcommand{\bY}{\mathbf{Y}} \newcommand{\bZ}{\mathbf{Z}} \newcommand{\calA}{\mathcal{A}} \newcommand{\calB}{\mathcal{B}} \newcommand{\calC}{\mathcal{C}} \newcommand{\calD}{\mathcal{D}} \newcommand{\calE}{\mathcal{E}} \newcommand{\calF}{\mathcal{F}} \newcommand{\calG}{\mathcal{G}} \newcommand{\calH}{\mathcal{H}} \newcommand{\calI}{\mathcal{I}} \newcommand{\calJ}{\mathcal{J}} \newcommand{\calK}{\mathcal{K}} \newcommand{\calL}{\mathcal{L}} \newcommand{\calM}{\mathcal{M}} \newcommand{\calN}{\mathcal{N}} \newcommand{\calO}{\mathcal{O}} \newcommand{\calP}{\mathcal{P}} \newcommand{\calQ}{\mathcal{Q}} \newcommand{\calR}{\mathcal{R}} \newcommand{\calS}{\mathcal{S}} \newcommand{\calT}{\mathcal{T}} \newcommand{\calU}{\mathcal{U}} \newcommand{\calV}{\mathcal{V}} \newcommand{\calW}{\mathcal{W}} \newcommand{\calX}{\mathcal{X}} \newcommand{\calY}{\mathcal{Y}} \newcommand{\calZ}{\mathcal{Z}} \newcommand{\R}{\mathbb{R}} \newcommand{\C}{\mathbb{C}} \newcommand{\N}{\mathbb{N}} \newcommand{\Z}{\mathbb{Z}} \newcommand{\F}{\mathbb{F}} \newcommand{\Q}{\mathbb{Q}} \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min} \newcommand{\nnz}[1]{\mbox{nnz}(#1)} \newcommand{\dotprod}[2]{\langle #1, #2 \rangle} \newcommand{\ignore}[1]{} \let\Pr\relax \DeclareMathOperator*{\Pr}{\mathbf{Pr}} \newcommand{\E}{\mathbb{E}} \DeclareMathOperator*{\Ex}{\mathbf{E}} \DeclareMathOperator*{\Var}{\mathbf{Var}} \DeclareMathOperator*{\Cov}{\mathbf{Cov}} \DeclareMathOperator*{\stddev}{\mathbf{stddev}} \DeclareMathOperator*{\avg}{avg} \DeclareMathOperator{\poly}{poly} \DeclareMathOperator{\polylog}{polylog} \DeclareMathOperator{\size}{size} \DeclareMathOperator{\sgn}{sgn} \DeclareMathOperator{\dist}{dist} \DeclareMathOperator{\vol}{vol} \DeclareMathOperator{\spn}{span} \DeclareMathOperator{\supp}{supp} \DeclareMathOperator{\tr}{tr} \DeclareMathOperator{\Tr}{Tr} \DeclareMathOperator{\codim}{codim} \DeclareMathOperator{\diag}{diag} \newcommand{\PTIME}{\mathsf{P}} \newcommand{\LOGSPACE}{\mathsf{L}} \newcommand{\ZPP}{\mathsf{ZPP}} \newcommand{\RP}{\mathsf{RP}} \newcommand{\BPP}{\mathsf{BPP}} \newcommand{\P}{\mathsf{P}} \newcommand{\NP}{\mathsf{NP}} \newcommand{\TC}{\mathsf{TC}} \newcommand{\AC}{\mathsf{AC}} \newcommand{\SC}{\mathsf{SC}} \newcommand{\SZK}{\mathsf{SZK}} \newcommand{\AM}{\mathsf{AM}} \newcommand{\IP}{\mathsf{IP}} \newcommand{\PSPACE}{\mathsf{PSPACE}} \newcommand{\EXP}{\mathsf{EXP}} \newcommand{\MIP}{\mathsf{MIP}} \newcommand{\NEXP}{\mathsf{NEXP}} \newcommand{\BQP}{\mathsf{BQP}} \newcommand{\distP}{\mathsf{dist\textbf{P}}} \newcommand{\distNP}{\mathsf{dist\textbf{NP}}} \newcommand{\eps}{\epsilon} \newcommand{\lam}{\lambda} \newcommand{\dleta}{\delta} \newcommand{\simga}{\sigma} \newcommand{\vphi}{\varphi} \newcommand{\la}{\langle} \newcommand{\ra}{\rangle} \newcommand{\wt}[1]{\widetilde{#1}} \newcommand{\wh}[1]{\widehat{#1}} \newcommand{\ol}[1]{\overline{#1}} \newcommand{\ul}[1]{\underline{#1}} \newcommand{\ot}{\otimes} \newcommand{\zo}{\{0,1\}} \newcommand{\co}{:} %\newcommand{\co}{\colon} \newcommand{\bdry}{\partial} \newcommand{\grad}{\nabla} \newcommand{\transp}{^\intercal} \newcommand{\inv}{^{-1}} \newcommand{\symmdiff}{\triangle} \newcommand{\symdiff}{\symmdiff} \newcommand{\half}{\tfrac{1}{2}} \newcommand{\bbone}{\mathbbm 1} \newcommand{\Id}{\bbone} \newcommand{\SAT}{\mathsf{SAT}} \newcommand{\bcalG}{\boldsymbol{\calG}} \newcommand{\calbG}{\bcalG} \newcommand{\bcalX}{\boldsymbol{\calX}} \newcommand{\calbX}{\bcalX} \newcommand{\bcalY}{\boldsymbol{\calY}} \newcommand{\calbY}{\bcalY} \newcommand{\bcalZ}{\boldsymbol{\calZ}} \newcommand{\calbZ}{\bcalZ} $$ </div> <div class="publications"> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row no-count"> <div id="2209.03430v2" class="col-sm-12"> <div class="title"><h2> <a href="http://arxiv.org/abs/2209.03430v2" rel="external nofollow noopener" target="_blank"> Foundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions </a> </h2></div> <div class="author"> Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency</div> <div class="periodical"> <em></em> Sep 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2209.03430v2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2209.03430v2.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <h3>Paper Abstract</h3> <div class="abstract"> <p>Multimodal machine learning is a vibrant multi-disciplinary research field that aims to design computer agents with intelligent capabilities such as understanding, reasoning, and learning through integrating multiple communicative modalities, including linguistic, acoustic, visual, tactile, and physiological messages. With the recent interest in video understanding, embodied autonomous agents, text-to-image generation, and multisensor fusion in application domains such as healthcare and robotics, multimodal machine learning has brought unique computational and theoretical challenges to the machine learning community given the heterogeneity of data sources and the interconnections often found between modalities. However, the breadth of progress in multimodal research has made it difficult to identify the common themes and open questions in the field. By synthesizing a broad range of application domains and theoretical frameworks from both historical and recent perspectives, this paper is designed to provide an overview of the computational and theoretical foundations of multimodal machine learning. We start by defining three key principles of modality heterogeneity, connections, and interactions that have driven subsequent innovations, and propose a taxonomy of six core technical challenges: representation, alignment, reasoning, generation, transference, and quantification covering historical and recent trends. Recent technical achievements will be presented through the lens of this taxonomy, allowing researchers to understand the similarities and differences across new approaches. We end by motivating several open problems for future research as identified by our taxonomy.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">2209.03430v2</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Foundations and Trends in Multimodal Machine Learning: Principles,
                     Challenges, and Open Questions}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2209.03430v2}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.LG}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://arxiv.org/abs/2209.03430v2}</span><span class="p">,</span>
  <span class="na">file</span> <span class="p">=</span> <span class="s">{2209.03430v2.pdf}</span><span class="p">,</span>
  <span class="na">eprintnover</span> <span class="p">=</span> <span class="s">{2209.03430}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <div class="post"> <article class="post-content"> <div id="markdown-content"> <h3 id="three-important-things">Three Important Things</h3> <h4 id="1-a-new-taxonomy-for-multimodal-machine-learning-research">1. A New Taxonomy For Multimodal Machine Learning Research</h4> <p>The authors propose a new taxonomy for classifying multimodal machine learning research, categorized into 6 different categories given below:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/summaries/multimodal-taxonomy-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/summaries/multimodal-taxonomy-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/summaries/multimodal-taxonomy-1400.webp"></source> <img src="/assets/img/summaries/multimodal-taxonomy.webp" class="z-depth-1 center" width="600px" height="auto" style="object-fit: cover" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption"> Taxonomy of the 6 core challenges in multimodal machine learning </figcaption> </figure> <ol> <li>Representation: What representations to use to best learn from multi-modal inputs?</li> <li>Alignment: Capturing relationships between inputs from different modalities</li> <li>Reasoning: Inference on multi-modal data over multiple steps</li> <li>Generation: Generating new outputs of a particular modality</li> <li>Transference: Allowing different modalities to learn from each other</li> <li>Quantification: An empirical and theoretical approach to multimodal models by quantifying their information content and exploring the extent and presence of relationships between multimodal data</li> </ol> <h4 id="2-contrastive-learning">2. Contrastive Learning</h4> <p>Contrastive learning, which is where similar samples are encouraged to be closer in the representation space and dissimilar samples are pushed further apart, is a popular approach for learning interactions between multi-modal inputs.</p> <h4 id="3-attention-maps-for-intermediate-concepts">3. Attention Maps for Intermediate Concepts</h4> <p>When performing multi-step reasoning on multimodal data, attention maps are a popular choice as it is human-interpretable and sufficiently general.</p> <h3 id="most-glaring-deficiency">Most Glaring Deficiency</h3> <p>It’s rather hard to criticize a survey, but some questions I had were whether the authors were aware of any research directions that don’t fit anywhere in the taxonomy, or perhaps span several categories.</p> <p>It would have also been illuminating to include specific examples for the more abstract research questions for people like me who are less familiar with work in the space.</p> <h3 id="conclusions-for-future-work">Conclusions for Future Work</h3> <p>It might be helpful to see where work on multimodal machine learning falls in the taxonomy, to understand which dimension of the problem it is tackling, and what are the other dimensions left to view the problem in. Perhaps the same technique could be applicable across several of the challenge domains.</p> </div> </article> <p class="post-meta">Written 2023</p> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Hrishikesh Singh. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script> </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let theme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===theme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=document.querySelector(".navbar-collapse");e.classList.contains("show")&&e.classList.remove("show"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"Publications",description:"Papers under review/submission and pre-prints are available upon request.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-ml-paper-summaries",title:"ML Paper Summaries",description:"",section:"Navigation",handler:()=>{window.location.href="/summaries/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-notes",title:"Notes",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-sample-test",title:"Sample Test",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/sample/"}},{id:"post-reading",title:"Reading",description:"A personal reading list.",section:"Posts",handler:()=>{window.location.href="/blog/2024/library/"}},{id:"post-research-my-two-cents",title:"Research: My two cents",description:"These are the nuggets of wisdom I found in the corners of internet while doing my own stint of research. Enjoy.",section:"Posts",handler:()=>{window.location.href="/blog/2024/research/"}},{id:"post-how-to-write-a-review-paper",title:"How to write a review paper?",description:"A walk through guide for structuring your research",section:"Posts",handler:()=>{window.location.href="/blog/2024/review-paper/"}},{id:"post-dsa-templates-java",title:"DSA templates (Java)",description:"These are java code templates based on most common datastructures and algorithm problems.",section:"Posts",handler:()=>{window.location.href="/blog/2024/java-template/"}},{id:"post-pythonic-templates",title:"Pythonic Templates",description:"A collection of code templates for common patterns in data structures and algorithms",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-template/"}},{id:"post-dynamic-programming",title:"Dynamic Programming",description:"Setting up the foundations of Dynamic programming along with a generic framework to disintegrate the problem into components",section:"Posts",handler:()=>{window.location.href="/blog/2024/dynamic-prog/"}},{id:"post-deep-learning",title:"Deep Learning",description:"A curated list of resources for a comprehensive understanding of deep learning.  ",section:"Posts",handler:()=>{window.location.href="/blog/2024/deep-learning/"}},{id:"post-the-beauty-of-latex",title:"The Beauty of LaTeX",description:"When was the first time you had to use LaTeX? If you are like most people, it was probably suddenly forced upon you during your first math or CS class where you had to start writing proofs, with minimal guidance on how to get started. Unfortunately, this meant that while many people have good operational knowledge of LaTeX, there are still many small mistakes and best practices which are not followed, which are not corrected by TAs as they are either not severe enough to warrant a note, or perhaps even the TAs themselves are not aware of them.  In this post, we cover some common mistakes that are made by LaTeX practitioners (even in heavily cited papers), and how to address them.",section:"Posts",handler:()=>{window.location.href="/blog/2023/latex/"}},{id:"post-java-collections",title:"Java Collections",description:"A brief reference for Java collections usecases.",section:"Posts",handler:()=>{window.location.href="/blog/2023/java-collection/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"posts-java-collections",title:"Java Collections",description:"A brief reference for Java collections usecases.",section:"Posts",handler:()=>{window.location.href="/blog/2023/java-collection/"}},{id:"posts-the-beauty-of-latex",title:"The Beauty of LaTeX",description:"When was the first time you had to use LaTeX? If you are like most people, it was probably suddenly forced upon you during your first math or CS class where you had to start writing proofs, with minimal guidance on how to get started. Unfortunately, this meant that while many people have good operational knowledge of LaTeX, there are still many small mistakes and best practices which are not followed, which are not corrected by TAs as they are either not severe enough to warrant a note, or perhaps even the TAs themselves are not aware of them.  In this post, we cover some common mistakes that are made by LaTeX practitioners (even in heavily cited papers), and how to address them.",section:"Posts",handler:()=>{window.location.href="/blog/2023/latex/"}},{id:"posts-deep-learning",title:"Deep Learning",description:"A curated list of resources for a comprehensive understanding of deep learning.  ",section:"Posts",handler:()=>{window.location.href="/blog/2024/deep-learning/"}},{id:"posts-dynamic-programming",title:"Dynamic Programming",description:"Setting up the foundations of Dynamic programming along with a generic framework to disintegrate the problem into components",section:"Posts",handler:()=>{window.location.href="/blog/2024/dynamic-prog/"}},{id:"posts-pythonic-templates",title:"Pythonic Templates",description:"A collection of code templates for common patterns in data structures and algorithms",section:"Posts",handler:()=>{window.location.href="/blog/2024/python-template/"}},{id:"posts-dsa-templates-java",title:"DSA templates (Java)",description:"These are java code templates based on most common datastructures and algorithm problems.",section:"Posts",handler:()=>{window.location.href="/blog/2024/java-template/"}},{id:"posts-how-to-write-a-review-paper",title:"How to write a review paper?",description:"A walk through guide for structuring your research",section:"Posts",handler:()=>{window.location.href="/blog/2024/review-paper/"}},{id:"posts-research-my-two-cents",title:"Research: My two cents",description:"These are the nuggets of wisdom I found in the corners of internet while doing my own stint of research. Enjoy.",section:"Posts",handler:()=>{window.location.href="/blog/2024/research/"}},{id:"posts-reading",title:"Reading",description:"A personal reading list.",section:"Posts",handler:()=>{window.location.href="/blog/2024/library/"}},{id:"posts-sample-test",title:"Sample Test",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/sample/"}},{id:"projects-deepglobe-land-cover-classification",title:"DeepGlobe Land Cover Classification",description:"Classifying land coverage using remote sensing satelite data.",section:"Projects",handler:()=>{window.location.href="/projects/DeepGlobe/"}},{id:"projects-gitlet",title:"GitLet",description:"A version control with core functionalities of git, written in Java.",section:"Projects",handler:()=>{window.location.href="/projects/GitLet/"}},{id:"projects-blight-detection",title:"Blight Detection",description:"Assisting the farmers by potato blight grade detection.",section:"Projects",handler:()=>{window.location.href="/projects/blight/"}},{id:"projects-driver-drowsiness-detection-system",title:"Driver Drowsiness Detection System",description:"Determining the distracted or drowsiness state of drivers based on live facial features.",section:"Projects",handler:()=>{window.location.href="/projects/ddds/"}},{id:"projects-classifier-from-scratch",title:"Classifier from Scratch",description:"Designing a linear classifier from scratch.",section:"Projects",handler:()=>{window.location.href="/projects/lin-class/"}},{id:"projects-market-orderbook",title:"Market Orderbook",description:"Simulation",section:"Projects",handler:()=>{window.location.href="/projects/market/"}},{id:"projects-know-your-rice",title:"Know your Rice \ud83c\udf5a",description:"Rice classification using Deep learning.",section:"Projects",handler:()=>{window.location.href="/projects/rice/"}},{id:"projects-vinci",title:"Vinci",description:"A Java Genetic Algorithm implementation with a focus on ease of use and extensibility.",section:"Projects",handler:()=>{window.location.href="/projects/vinci/"}},{id:"summaries-deep-contextualized-word-representations-elmo",title:"Deep contextualized word representations (ELMo)",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-02-deep-contextualized-word-representations/"}},{id:"summaries-foundations-and-trends-in-multimodal-machine-learning-principles-challenges-and-open-questions",title:"Foundations and Trends in Multimodal Machine Learning: Principles,  Challenges, and Open Questions",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-02-foundations-and-trends-in-multimodal-machine-learning-principles--challenges-and-open-questions/"}},{id:"summaries-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding",title:"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-03-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding/"}},{id:"summaries-chain-of-thought-prompting-elicits-reasoning-in-large-language-models",title:"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-03-chain-of-thought-prompting-elicits-reasoning-in-large-language-models/"}},{id:"summaries-training-language-models-to-follow-instructions-with-human-feedback-instructgpt",title:"Training language models to follow instructions with human feedback (InstructGPT)",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-05-training-language-models-to-follow-instructions-with-human-feedback/"}},{id:"summaries-evaluating-large-language-models-trained-on-code-codex",title:"Evaluating Large Language Models Trained on Code (Codex)",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-06-evaluating-large-language-models-trained-on-code/"}},{id:"summaries-bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension",title:"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-09-bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension/"}},{id:"summaries-dense-passage-retrieval-for-open-domain-question-answering",title:"Dense Passage Retrieval for Open-Domain Question Answering",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-10-dense-passage-retrieval-for-open-domain-question-answering/"}},{id:"summaries-language-models-are-unsupervised-multitask-learners-gpt-2",title:"Language Models are Unsupervised Multitask Learners (GPT-2)",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-10-language-models-are-unsupervised-multitask-learners/"}},{id:"summaries-simple-synthetic-data-reduces-sycophancy-in-large-language-models",title:"Simple synthetic data reduces sycophancy in large language models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-10-simple-synthetic-data-reduces-sycophancy-in-large-language-models/"}},{id:"summaries-generative-agents-interactive-simulacra-of-human-behavior",title:"Generative Agents: Interactive Simulacra of Human Behavior",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-11-generative-agents-interactive-simulacra-of-human-behavior/"}},{id:"summaries-improving-language-understanding-by-generative-pre-training-gpt",title:"Improving Language Understanding by Generative Pre-Training (GPT)",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-11-improving-language-understanding-by-generative-pre-training/"}},{id:"summaries-metagpt-meta-programming-for-multi-agent-collaborative-framework",title:"MetaGPT: Meta Programming for Multi-Agent Collaborative Framework",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-11-metagpt-meta-programming-for-multi-agent-collaborative-framework/"}},{id:"summaries-transformers-in-speech-processing-a-survey",title:"Transformers in Speech Processing: A Survey",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-19-transformers-in-speech-processing-a-survey/"}},{id:"summaries-accurate-detection-of-wake-word-start-and-end-using-a-cnn",title:"Accurate Detection of Wake Word Start and End Using a CNN",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-22-accurate-detection-of-wake-word-start-and-end-using-a-cnn/"}},{id:"summaries-exploring-the-limits-of-transfer-learning-with-a-unified-text-to-text-transformer",title:"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-22-exploring-the-limits-of-transfer-learning-with-a-unified-text-to-text-transformer/"}},{id:"summaries-efficiently-modeling-long-sequences-with-structured-state-spaces",title:"Efficiently Modeling Long Sequences with Structured State Spaces",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-25-efficiently-modeling-long-sequences-with-structured-state-spaces/"}},{id:"summaries-a-watermark-for-large-language-models",title:"A Watermark for Large Language Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-27-a-watermark-for-large-language-models/"}},{id:"summaries-extracting-training-data-from-large-language-models",title:"Extracting Training Data from Large Language Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-08-29-extracting-training-data-from-large-language-models/"}},{id:"summaries-loss-landscapes-and-optimization-in-over-parameterized-non-linear-systems-and-neural-networks",title:"Loss Landscapes and Optimization in Over-Parameterized Non-Linear Systems and Neural Networks",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-09-04-loss-landscapes-and-optimization-in-over-parameterized-non-linear-systems-and-neural-networks/"}},{id:"summaries-gradient-descent-provably-optimizes-over-parameterized-neural-networks",title:"Gradient Descent Provably Optimizes Over-parameterized Neural Networks",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-09-07-gradient-descent-provably-optimizes-over-parameterized-neural-networks/"}},{id:"summaries-the-implicit-bias-of-gradient-descent-on-separable-data",title:"The Implicit Bias of Gradient Descent on Separable Data",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-09-09-the-implicit-bias-of-gradient-descent-on-separable-data/"}},{id:"summaries-understanding-deep-learning-requires-rethinking-generalization",title:"Understanding Deep Learning Requires Rethinking Generalization",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-09-10-understanding-deep-learning-requires-rethinking-generalization/"}},{id:"summaries-calibrate-before-use-improving-few-shot-performance-of-language-models",title:"Calibrate Before Use: Improving Few-Shot Performance of Language Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-09-23-calibrate-before-use-improving-few-shot-performance-of-language-models/"}},{id:"summaries-rethinking-the-role-of-demonstrations-what-makes-in-context-learning-work",title:"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-09-24-rethinking-the-role-of-demonstrations-what-makes-in-context-learning-work/"}},{id:"summaries-repository-level-prompt-generation-for-large-language-models-of-code",title:"Repository-Level Prompt Generation for Large Language Models of Code",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-09-26-repository-level-prompt-generation-for-large-language-models-of-code/"}},{id:"summaries-on-the-dangers-of-stochastic-parrots-can-language-models-be-too-big",title:"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-10-04-on-the-dangers-of-stochastic-parrots-can-language-models-be-too-big/"}},{id:"summaries-an-image-is-worth-one-word-personalizing-text-to-image-generation-using-textual-inversion",title:"An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-10-15-an-image-is-worth-one-word-personalizing-text-to-image-generation-using-textual-inversion/"}},{id:"summaries-instructpix2pix-learning-to-follow-image-editing-instructions",title:"InstructPix2Pix: Learning to Follow Image Editing Instructions",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-10-17-instructpix2pix-learning-to-follow-image-editing-instructions/"}},{id:"summaries-zero-shot-image-to-image-translation",title:"Zero-shot Image-to-Image Translation",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-10-22-zero-shot-image-to-image-translation/"}},{id:"summaries-universal-and-transferable-adversarial-attacks-on-aligned-language-models",title:"Universal and Transferable Adversarial Attacks on Aligned Language Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-10-31-universal-and-transferable-adversarial-attacks-on-aligned-language-models/"}},{id:"summaries-high-resolution-image-synthesis-with-latent-diffusion-models",title:"High-Resolution Image Synthesis with Latent Diffusion Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-11-03-high-resolution-image-synthesis-with-latent-diffusion-models/"}},{id:"summaries-large-language-models-for-software-engineering-survey-and-open-problems",title:"Large Language Models for Software Engineering: Survey and Open Problems",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2023-12-12-large-language-models-for-software-engineering-survey-and-open-problems/"}},{id:"summaries-matryoshka-representation-learning",title:"Matryoshka Representation Learning",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2024-02-19-matryoshka-representation-learning/"}},{id:"summaries-colbert-efficient-and-effective-passage-search-via-contextualized-late-interaction-over-bert",title:"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2024-02-22-colbert-efficient-and-effective-passage-search-via-contextualized-late-interaction-over-bert/"}},{id:"summaries-dspy-compiling-declarative-language-model-calls-into-self-improving-pipelines",title:"DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2024-03-15-dspy-compiling-declarative-language-model-calls-into-self-improving-pipelines/"}},{id:"summaries-scaling-laws-for-neural-language-models",title:"Scaling Laws for Neural Language Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2024-03-23-scaling-laws-for-neural-language-models/"}},{id:"summaries-training-compute-optimal-large-language-models",title:"Training Compute-Optimal Large Language Models",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2024-03-23-training-compute-optimal-large-language-models/"}},{id:"summaries-longrope-extending-llm-context-window-beyond-2-million-tokens",title:"LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens",description:"",section:"Summaries",handler:()=>{window.location.href="/summaries/2024-03-27-longrope-extending-llm-context-window-beyond-2-million-tokens/"}},{id:"work-wordless-ink-on-paper",title:"Wordless Ink on Paper",description:"",section:"Work",handler:()=>{window.location.href="/work/art/"}},{id:"work-machine-learning-research-internship-iit-delhi",title:"Machine Learning Research Internship @ IIT-Delhi",description:"",section:"Work",handler:()=>{window.location.href="/work/iitd/"}},{id:"work-rese",title:"Rese.",description:"",section:"Work",handler:()=>{window.location.href="/work/iitr/"}},{id:"work-research-statement",title:"Research Statement",description:"",section:"Work",handler:()=>{window.location.href="/work/research-statement/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%68%72%69%73%68%69%6B%65%73%68.%68%73%6B@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=ogGhORwAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/hrishikeshh","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/hrishikesh-singh","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> </body> </html>